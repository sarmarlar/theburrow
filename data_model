

def prepend_ones_column(A):

    '''

    Add a ones column to the left side of an array



    Inputs: 

        A: a numpy array



    Output: a numpy array

    '''

    ones_col = np.ones((A.shape[0], 1))

    return np.hstack([ones_col, A])





def linear_regression(X, y):

    '''

    Compute linear regression. Finds model, beta, that minimizes

    X*beta - Y in a least squared sense.



    Accepts inputs with type array

    Returns beta, which is used only by apply_beta



    Examples

    --------

    >>> X = np.array([[5, 2], [3, 2], [6, 2.1], [7, 3]]) # predictors

    >>> y = np.array([5, 2, 6, 6]) # dependent

    >>> beta = linear_regression(X, y)  # compute the coefficients

    >>> beta

    array([ 1.20104895,  1.41083916, -1.6958042 ])

    >>> apply_beta(beta, X) # apply the function defined by beta

    array([ 4.86363636,  2.04195804,  6.1048951 ,  5.98951049])

    '''

    assert_Xy(X, y, fname='linear_regression')



    X_with_ones = prepend_ones_column(X)



    # Do actual computation

    beta = np.linalg.lstsq(X_with_ones, y)[0]



    return beta





def apply_beta(beta, X):

    '''

    Apply beta, the function generated by linear_regression, to the

    specified values



    Inputs:

        model: beta as returned by linear_regression

        Xs: 2D array of floats



    Returns:

        result of applying beta to the data, as an array.



        Given:

            beta = array([B0, B1, B2,...BK])

            Xs = array([[x11, x12, ..., x0K],

                        [x21, x22, ..., x1K],

                        ...

                        [xN1, xN2, ..., xNK]])



            result will be:

            array([B0+B1*x11+B2*x12+...+BK*x1K,

                   B0+B1*x21+B2*x22+...+BK*x2K,

                   ...

                   B0+B1*xN1+B2*xN2+...+BK*xNK])

    '''

    assert_Xbeta(X, beta, fname='apply_beta')



    # Add a column of ones

    X_incl_ones = prepend_ones_column(X)



    # Calculate X*beta

    yhat = np.dot(X_incl_ones, beta)

    return yhat





def read_file(filename):

    '''

    Read data from the specified file.  Split the lines and convert

    float strings into floats.  Assumes the first row contains labels

    for the columns.



    Inputs:

      filename: name of the file to be read



    Returns:

      (list of strings, 2D array)

    '''

    with open(filename) as f:

        labels = f.readline().strip().split(',')

        data = np.loadtxt(f, delimiter=',', dtype=np.float64)

        return labels, data





###############

#             #

#  Your code  #

#             #

###############



def variance(y):

    '''

    Calculate the variance of the dependent variable (represented by a column of the data).  



    Inputs: 

        y: a column of the data representing the dependent variable. 



    Returns: 

        The variance of the dependent variable.  

    '''



    y_bar = np.mean(y)

    N = len(y)

    squared_difference_list = [((yn - y_bar) ** 2) for yn in y]

    variance_y = 1/N * sum(squared_difference_list)



    return variance_y





def Calc_R_squared(X, y):

    '''

    Calculate the R squared value for given predictor variables and a given dependent varible. 



    Inputs: 

        X: predictor variable columns (2D array)

        y: dependent variable column (1D array)



    Return: 

        Value for R squared.  

    ''' 

    y_bar = np.mean(y)



    ydiffs = (y - y_bar) ** 2

    sum_mean_diff = sum(ydiffs)



    beta = linear_regression(X, y)

    yhat = apply_beta(beta, X)

    ydiffs = (y - yhat) ** 2

    sum_hat_diff = sum(ydiffs)



    R_squared = 1 - (sum_hat_diff)/(sum_mean_diff)



    return (R_squared, beta)



def Calc_R_squared_testing(X_training, X_testing, y):

    '''

    Calculate the R squared value for given predictor variables from the training data, predictor variables from the testing data, 

    and a given dependent varible. 



    Inputs: 

        X: predictor variable columns (2D array)

        y: dependent variable column (1D array)



    Return: 

        Value for R squared.  

    ''' 

    y_bar = np.mean(y)



    ydiffs = (y - y_bar) ** 2

    sum_mean_diff = sum(ydiffs)



    beta = linear_regression(X_training, y) #use training data to calculate beta 

    yhat = apply_beta(beta, X_testing) #use testing data to calculate y hat

    ydiffs = (y - yhat) ** 2

    sum_hat_diff = sum(ydiffs)



    R_squared = 1 - (sum_hat_diff)/(sum_mean_diff)



    return (R_squared)





def model_data(labels, data_array, predictor_column_indices, dependent_column_index): 

    '''

    Function that produces a data structure that encapsulates relevant information about the model.  



    Inputs:

        labels: variable labels (list of strings)

        data_array: array of data (2D array)

        list_predictor_columns: list of predictor column indices



    Outputs: 

        (X, y) tuple

        X: data array of predictor columns.

        y: data array of dependent column



    '''



    X = data_array[:,predictor_column_indices]

    y = data_array[:,dependent_column_index]



    return (X, y)





def get_best_R2(labels, data_array, predictor_column_indices, dependent_column_index):

    '''

    Function that gets the best R squared value. 



    Inputs:

        predictor_column_indices (list of integers)



    Outputs: 

        dependent column index (integer)



    '''

    best_R2 = 0

    best_i = -1

    beta_set = []



    for i in predictor_column_indices:

        new_R2 = Calc_R_squared(data_array[:,[i]], data_array[:,dependent_column_index])[0]

        beta = Calc_R_squared(data_array[:,[i]], data_array[:,dependent_column_index])[1]

        if new_R2 > best_R2: 

            best_R2 = new_R2

            best_i = i



    return (best_R2, best_i, beta_set)  





def fixed_k_model(labels, data_array, predictor_column_indices, dependent_column_index):

    '''

    Function that produces the top k predictor variables and their corresponding R squared value. 



    Inputs: 

        labels: variable labels (list of strings)

        data_array: array of data (2D array)

        predictor_column_indices: list of predictor column indices (list of integers)

        dependent_column_index: index of dependent column (integer)



    Outputs: 

        tuple: 

        best_predictor_indices_list: ordered list of best indicies for fixed k.  

        best_R2_list: ordered list of best R squared values for fixed k. 

    '''

    best_R2_list = []

    unused = predictor_column_indices[:]

    best_predictor_indices_list = []



    highest_R2 = 0 

    best_R2_info = get_best_R2(labels, data_array, predictor_column_indices, dependent_column_index)

    best_R2 = best_R2_info[0]

    best_k = best_R2_info[1]

    beta_set = best_R2_info[2]

    best_R2_list.append(best_R2)

    best_predictor_indices_list.append(best_k)

    unused.pop(best_k)



    while unused:



        for i in unused: 

            X = data_array[:,best_predictor_indices_list + [i]]

            y = data_array[:,dependent_column_index]

            new_R2 = Calc_R_squared(X, y)[0]

            if new_R2 > highest_R2:

                highest_R2 = new_R2

                best_i = i



       

        unused.pop(unused.index(best_i))

        best_predictor_indices_list.append(best_i)

        best_R2_list.append(highest_R2)



    return (best_predictor_indices_list, best_R2_list, beta_set)





def select_best_k(labels, data_array, predictor_column_indices, dependent_column_index, threshold): 

    '''

    Function that produces the top k predictor variables and their corresponding R squared value up to a specified threshold of diminishing returns. 



    Inputs: 

        labels: variable labels (list of strings)

        data_array: array of data (2D array)

        predictor_column_indices: list of predictor column indices (list of integers)

        dependent_column_index: index of dependent column (integer)

        threshold: specified threshold of dimishishing returns (float)



    Output:

        tuple of the index of the stopping point and the R squared value associated with the stopping point.  

    '''



    k_model = fixed_k_model(labels, data_array, predictor_column_indices, dependent_column_index)

    R2_list = []



    for R2 in k_model[1]:

        R2_list.append(R2)

        if len(R2_list) > 1:

            diff = R2_list[-1] - R2_list[-2]

            if diff < threshold:

                stop_R2 = R2_list[-2]

                stop_index = k_model[1].index(stop_R2)

                break 



    return (stop_index, stop_R2)

        
